# ================================================================
# HCLS AI Factory - Environment Configuration
# ================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# NEVER commit the .env file - it's gitignored for your protection
# ================================================================

# ================================================================
# NVIDIA NGC (Required for Parabricks and BioNeMo)
# ================================================================
# Get your API key: https://ngc.nvidia.com/setup/api-key
NGC_API_KEY=your-ngc-api-key-here

# ================================================================
# Anthropic (Required for RAG/Chat pipeline)
# ================================================================
# Get your API key: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# ================================================================
# HuggingFace (Optional - for local LLM models)
# ================================================================
# Get your token: https://huggingface.co/settings/tokens
HF_TOKEN=your-huggingface-token-here

# ================================================================
# LLM Configuration
# ================================================================
# Provider: anthropic, ollama, openai, vllm
# Default in code is ollama (local LLM for ARM64/DGX Spark).
# Set to anthropic if using Claude API.
LLM_PROVIDER=anthropic
LLM_MODEL=claude-sonnet-4-20250514

# For local LLM on DGX Spark (default if LLM_PROVIDER not set):
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:70b
# OLLAMA_HOST=http://localhost:11434

# For vLLM on x86 systems:
# LLM_PROVIDER=vllm
# VLLM_HOST=localhost
# VLLM_PORT=8080
# VLLM_MODEL=meta-llama/Llama-3.1-70B-Instruct

# ================================================================
# Milvus Vector Database
# ================================================================
MILVUS_HOST=localhost
MILVUS_PORT=19530

# ================================================================
# BioNeMo Services
# ================================================================
# NIM Mode: "cloud" uses NVIDIA hosted APIs (ARM64 compatible)
#           "local" uses Docker containers (x86_64 only)
NIM_MODE=cloud
NVIDIA_API_KEY=your-nvidia-api-key-here

# Cloud NIM endpoints (default for NIM_MODE=cloud)
MOLMIM_URL=https://health.api.nvidia.com/v1/biology/nvidia/molmim/generate
DIFFDOCK_URL=https://health.api.nvidia.com/v1/biology/mit/diffdock

# Local NIM endpoints (for NIM_MODE=local)
# MOLMIM_URL=http://localhost:8001
# DIFFDOCK_URL=http://localhost:8002

# Mock fallback for demo without NIM services
# NIM_ALLOW_MOCK_FALLBACK=true

# ================================================================
# RAG Settings
# ================================================================
RAG_TOP_K=10
RAG_SCORE_THRESHOLD=0.5
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7

# ================================================================
# Monitoring (Optional)
# ================================================================
GRAFANA_USER=admin
GRAFANA_PASSWORD=change-this-password

# ================================================================
# Intelligence Agents
# ================================================================
# Agent ports (defaults shown â€” override if needed)
CART_AGENT_PORT=8521
IMAGING_AGENT_PORT=8525
ONCO_AGENT_PORT=8526

# Cross-modal threshold for agent evidence linking (0.0-1.0)
CROSS_MODAL_THRESHOLD=0.7

# Precision Oncology Agent settings
ONCO_CORS_ORIGINS=http://localhost:8526,http://localhost:8527
ONCO_MAX_REQUEST_SIZE_MB=50

# Imaging Agent NIM services (shared with BioNeMo config above)
# VISTA3D_URL=http://localhost:8003
# MAISI_URL=http://localhost:8004

# ================================================================
# Data Paths
# ================================================================
VCF_INPUT_PATH=/path/to/your/variants.vcf.gz
